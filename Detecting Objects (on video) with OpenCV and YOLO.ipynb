{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detectiong Objects (on video) with OpenCV and YOLO v3 in Python\n",
    "\n",
    "**References** \n",
    "\n",
    "* [Deep Learning based Object Detection using YOLOv3 with OpenCV](https://www.learnopencv.com/deep-learning-based-object-detection-using-yolov3-with-opencv-python-c/) \n",
    "* [YOLO Object Detection with OpenCV and Python](https://www.arunponnusamy.com/yolo-object-detection-opencv-python.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n",
      "OpenCV version: 3.4.3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib\n",
    "\n",
    "print( \"OpenCV version:\", cv2.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLO files\n",
    "\n",
    "The files used here were download from:\n",
    " * configuration file: https://github.com/pjreddie/darknet/blob/master/cfg/yolov3.cfg?raw=true\n",
    " * pre-trained weights file: https://pjreddie.com/media/files/yolov3.weights\n",
    " * text file containing class names: https://github.com/pjreddie/darknet/blob/master/data/coco.names?raw=true\n",
    " \n",
    "You can use the method `filename = wget.download(url)` to downloand them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to yolo configuration file\n",
    "model_configuration = \"../yolo_files/yolov3.cfg\"\n",
    "\n",
    "# Path to yolo pre-trained weights\n",
    "model_weights = \"../yolo_files/yolov3.weights\"\n",
    "\n",
    "# Path to text file containing class names\n",
    "classes_file = \"../yolo_files/coco.names\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read class names from text file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = None\n",
    "with open(classes_file, \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['person', 'bicycle', 'car', 'motorbike', 'aeroplane'] of 80\n"
     ]
    }
   ],
   "source": [
    "print( classes[:5], \"of\", len(classes) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate different colors for different classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = np.random.uniform( 0, 255, size = ( len(classes), 3 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.48137798e+02, 8.12286869e-02, 1.02033610e+02],\n",
       "       [2.52689469e+02, 2.21725636e+02, 2.25909836e+02],\n",
       "       [4.50061552e+01, 2.07078898e+02, 2.09335059e+02]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colors[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read pre-trained model and config file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = cv2.dnn.readNetFromDarknet( model_configuration, model_weights)\n",
    "net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary functions\n",
    "\n",
    "**Drawing boundin box**\n",
    "\n",
    "`draw_bounding_box()` function draws rectangle over the given predicted region and writes class name and confidence value over the box. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to draw bounding box on the detected object with class name\n",
    "def draw_bounding_box(img, class_id, confidence, x, y, x_plus_w, y_plus_h):\n",
    "    \n",
    "    label = '%.2f' % confidence\n",
    "    color = colors[class_id]\n",
    "         \n",
    "    # Get the label for the class name and its confidence\n",
    "    if classes:\n",
    "        assert(class_id < len(classes))\n",
    "        label = '%s:%s' % (classes[class_id], label)\n",
    "        \n",
    "    # Draw a bounding box.\n",
    "    cv2.rectangle(img, (x, y), (x_plus_w, y_plus_h), color, 2)\n",
    "    \n",
    "    #Display the label at the top of the bounding box    \n",
    "    cv2.putText(img, label, (x-10,y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Post-processing the networkâ€™s output**\n",
    "\n",
    "`post_process()` function remove the bounding boxes with low confidence using non-maxima suppression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process( frame, outputs,  conf_threshold = 0.5, nms_threshold = 0.4):\n",
    "    frame_height, frame_width = frame.shape[:2]\n",
    "    \n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    bounding_boxes = []\n",
    "    \n",
    "    # For each detetion from each output layer get the confidence, \n",
    "    # class id, bounding box params and ignore weak detections\n",
    "    for out in outputs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax( scores )\n",
    "            conf = scores[ class_id ]\n",
    "            if conf > conf_threshold:\n",
    "                center_x = int(detection[0] * frame_width)\n",
    "                center_y = int(detection[1] * frame_height)\n",
    "                w = int(detection[2] * frame_width)\n",
    "                h = int(detection[3] * frame_height)\n",
    "                x = center_x - w / 2\n",
    "                y = center_y - h / 2\n",
    "\n",
    "                class_ids.append( class_id )\n",
    "                confidences.append( float(conf) )\n",
    "                bounding_boxes.append( [x, y, w, h] )\n",
    "    \n",
    "    # Perform non maximum suppression to eliminate redundant overlapping boxes with\n",
    "    # lower confidences.\n",
    "    indices = cv2.dnn.NMSBoxes( bounding_boxes, confidences, conf_threshold, nms_threshold )\n",
    "\n",
    "    frame_out = frame.copy()\n",
    "        \n",
    "    for i in indices:\n",
    "        i = i[0]\n",
    "        x, y, w, h = bounding_boxes[i]\n",
    "        frame_out = draw_bounding_box( frame_out, class_ids[i], confidences[i], \n",
    "                                       round(x), round(y), round(x+w), round(y+h) )\n",
    "    \n",
    "    return frame_out   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output layer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally in a sequential CNN network there will be only one output layer at the end. In the YOLO v3 architecture we are using there are multiple output layers giving out predictions. `get_output_layers()` function gives the names of the output layers. An output layer is not connected to any next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get output layers names in the architecture\n",
    "def get_output_layers( net ):\n",
    "    layer_names = net.getLayerNames( )\n",
    "    output_layers = [ layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers() ]\n",
    "    return output_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yolo_82', 'yolo_94', 'yolo_106']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_output_layers( net )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputFile = \"../data/run.mp4\"\n",
    "outputFile = \"yolo_out_py.avi\"\n",
    "\n",
    "inpWidth = 416       #Width of network's input image\n",
    "inpHeight = 416      #Height of network's input image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done processing !!!\n",
      "Output file is stored as  ../data/run_yolo_out_py.avi\n",
      "--- 166.96527457237244 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Open the video file\n",
    "if os.path.isfile( inputFile ):\n",
    "    \n",
    "    cap = cv2.VideoCapture( inputFile )\n",
    "    outputFile = inputFile[:-4]+'_yolo_out_py.avi'\n",
    "    \n",
    "    video_width = round( cap.get(cv2.CAP_PROP_FRAME_WIDTH) )\n",
    "    video_height = round( cap.get(cv2.CAP_PROP_FRAME_HEIGHT) )\n",
    "    vid_writer = cv2.VideoWriter( outputFile, cv2.VideoWriter_fourcc('M','J','P','G'), 30, (video_width, video_height) )\n",
    "else:\n",
    "    print(\"Input image file \", inputFile, \" doesn't exist\") \n",
    "\n",
    "start_time = time.time()\n",
    "print( \"Processing...\" )\n",
    "\n",
    "# get frame from the video\n",
    "hasFrame, frame = cap.read()\n",
    "\n",
    "while hasFrame:\n",
    "    # Create a 4D blob from a frame.\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1/255, (inpWidth, inpHeight), [0,0,0], 1, crop=False)\n",
    " \n",
    "    # Sets the input to the network\n",
    "    net.setInput(blob)\n",
    " \n",
    "    # Runs the forward pass to get output of the output layers\n",
    "    outs = net.forward( get_output_layers(net) )\n",
    " \n",
    "    # Remove the bounding boxes with low confidence\n",
    "    frame = post_process(frame, outs)\n",
    "    \n",
    "    # Put efficiency information. The function getPerfProfile returns the \n",
    "    # overall time for inference(t) and the timings for each of the layers(in layersTimes)\n",
    "    t, _ = net.getPerfProfile()\n",
    "    label = 'Inference time: %.2f ms' % (t * 1000.0 / cv2.getTickFrequency())\n",
    "    cv2.putText(frame, label, (0, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255))\n",
    "    \n",
    "    vid_writer.write(frame.astype(np.uint8))\n",
    "    \n",
    "    hasFrame, frame = cap.read()\n",
    "\n",
    "# Release device\n",
    "cap.release()\n",
    "\n",
    "print( \"Done processing !!!\" )\n",
    "print( \"Output file is stored as \", outputFile )\n",
    "print( \"--- %s seconds ---\" % (time.time() - start_time) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outputs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../data/run_yolo_out.PNG\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../data/pigeons_yolo_out.PNG\" width=\"800\" />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (MLOpenHack)",
   "language": "python",
   "name": "mlopenhack"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
